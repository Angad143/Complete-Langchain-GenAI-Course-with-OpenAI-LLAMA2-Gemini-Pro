{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction of LangChain:**\n",
    "\n",
    "- **LangChain** is a framework designed to make it easier to build applications using large language models (LLMs) by integrating them with external data, memory, and APIs. \n",
    "- It enables the creation of **chains** (sequences of tasks), **agents** (decision-making LLMs), and **memory** (tracking conversation context). \n",
    "- LangChain is useful for tasks like conversational agents, automated workflows, and document-based question-answering.\n",
    "\n",
    "#### **Key Components**:\n",
    "   - **Chains**: Create sequences of tasks, allowing integration with multiple LLMs or tools in a pipeline.\n",
    "   - **Agents**: Enable LLMs to make dynamic decisions and select appropriate tools or APIs autonomously.\n",
    "   - **Memory**: Track conversation context across interactions for personalized and context-aware responses.\n",
    "   - **Integrations**: Works with APIs like OpenAI, Hugging Face, and Google Cloud, making it versatile for LLM use.\n",
    "   - **Prompt Templates**: Structure input queries for consistency when communicating with LLMs.\n",
    "\n",
    "#### **Use Cases**:\n",
    "   - **Conversational Agents**: Build chatbots that retain context and make decisions.\n",
    "   - **Document Question-Answering**: Fetch and process documents before answering queries.\n",
    "   - **Automated Workflows**: Integrate LLMs with tools to automate tasks like web scraping or API processing.\n",
    "\n",
    "LangChain is ideal for developers creating AI applications that require more than simple text generation, especially those needing external data or tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "# Set the OpenAI API key in the environment variable\n",
    "os.environ[\"OPEN_API_KEY\"] = \"openai_api_key\"\n",
    "\n",
    "# Initialize OpenAI LLM with the API key and set temperature to control randomness in response\n",
    "openai_llm = OpenAI(openai_api_key = os.environ[\"OPEN_API_KEY\"], temperature = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input text\n",
    "text = \"What is the capital of India\"\n",
    "\n",
    "# Use the LLM to generate a response for the input question and print it\n",
    "print(openai_llm.predict(text))\n",
    "\n",
    "# Expected Output: \"The capital of India is New Delhi.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Import Hugging Face Hub from LangChain\n",
    "from langchain import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "# Set the Hugging Face API token in the environment variable\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"huggingfacehub api token\"\n",
    "\n",
    "# Initialize Hugging Face Hub LLM with the specified repo and model configuration\n",
    "huggingface_llm = HuggingFaceHub(repo_id = \"google/flan-t5-large\", model_kwargs = {\"temperature\": 0, \"max_length\": 64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moscow\n"
     ]
    }
   ],
   "source": [
    "# Generate a response for a new question using Hugging Face and print it\n",
    "output = huggingface_llm.predict(\"Can you tell me the capital of Russia\")\n",
    "print(output)\n",
    "# Expected Output: \"Moscow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love\n"
     ]
    }
   ],
   "source": [
    "# Generate a response asking for a poem using Hugging Face and print the result\n",
    "output = huggingface_llm.predict(\"Can you write a poem about AI\")\n",
    "print(output)\n",
    "# Expected Output: (A short, generated poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenAI LLM to generate a poem response and print it\n",
    "openai_llm_output = openai_llm.predict(\"Can you write a poem about AI\")\n",
    "print(openai_llm_output)\n",
    "# Expected Output: (Another poem, possibly longer and more structured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PromptTemplate and LLMChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PromptTemplate:**\n",
    "- A PromptTemplate in LangChain is a tool for creating dynamic and structured prompts for language models. \n",
    "- It allows you to define placeholders for variables that can be replaced with user-provided input, ensuring consistency in how prompts are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me the capital of this India\n"
     ]
    }
   ],
   "source": [
    "# Importing PromptTemplate from LangChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template that expects a variable 'country' to be provided\n",
    "prompt_template = PromptTemplate(input_variables=['country'], template=\"Tell me the capital of this {country}\")\n",
    "\n",
    "# Use the prompt template to format the input with \"India\" as the country\n",
    "formatted_prompt = prompt_template.format(country=\"India\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LLMChain:**\n",
    "- LLMChain is a component that combines a language model (LLM) and a PromptTemplate to create a streamlined process for generating responses. \n",
    "- It takes in a user input, formats it with the prompt template, passes it to the LLM, and returns the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kathmandu\n"
     ]
    }
   ],
   "source": [
    "# Import LLMChain from LangChain, which links the LLM and the prompt template\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create an LLMChain object using the OpenAI LLM and the prompt template\n",
    "chain = LLMChain(llm=huggingface_llm, prompt=prompt_template)\n",
    "\n",
    "# Run the chain with \"India\" as input and print the response\n",
    "print(chain.run(\"Nepal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
