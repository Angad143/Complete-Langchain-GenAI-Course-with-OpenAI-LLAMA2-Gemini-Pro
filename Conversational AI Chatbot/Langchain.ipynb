{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction of LangChain:**\n",
    "\n",
    "- **LangChain** is a framework designed to make it easier to build applications using large language models (LLMs) by integrating them with external data, memory, and APIs. \n",
    "- It enables the creation of **chains** (sequences of tasks), **agents** (decision-making LLMs), and **memory** (tracking conversation context). \n",
    "- LangChain is useful for tasks like conversational agents, automated workflows, and document-based question-answering.\n",
    "\n",
    "#### **Key Components**:\n",
    "   - **Chains**: Create sequences of tasks, allowing integration with multiple LLMs or tools in a pipeline.\n",
    "   - **Agents**: Enable LLMs to make dynamic decisions and select appropriate tools or APIs autonomously.\n",
    "   - **Memory**: Track conversation context across interactions for personalized and context-aware responses.\n",
    "   - **Integrations**: Works with APIs like OpenAI, Hugging Face, and Google Cloud, making it versatile for LLM use.\n",
    "   - **Prompt Templates**: Structure input queries for consistency when communicating with LLMs.\n",
    "\n",
    "#### **Use Cases**:\n",
    "   - **Conversational Agents**: Build chatbots that retain context and make decisions.\n",
    "   - **Document Question-Answering**: Fetch and process documents before answering queries.\n",
    "   - **Automated Workflows**: Integrate LLMs with tools to automate tasks like web scraping or API processing.\n",
    "\n",
    "LangChain is ideal for developers creating AI applications that require more than simple text generation, especially those needing external data or tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "# Set the OpenAI API key in the environment variable\n",
    "os.environ[\"OPEN_API_KEY\"] = \"openai_api_key\"\n",
    "\n",
    "# Initialize OpenAI LLM with the API key and set temperature to control randomness in response\n",
    "openai_llm = OpenAI(openai_api_key = os.environ[\"OPEN_API_KEY\"], temperature = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input text\n",
    "text = \"What is the capital of India\"\n",
    "\n",
    "# Use the LLM to generate a response for the input question and print it\n",
    "print(openai_llm.predict(text))\n",
    "\n",
    "# Expected Output: \"The capital of India is New Delhi.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Hugging Face Hub from LangChain\n",
    "from langchain import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "# Set the Hugging Face API token in the environment variable\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"huggingfacehub api token\"\n",
    "\n",
    "# Initialize Hugging Face Hub LLM with the specified repo and model configuration\n",
    "huggingface_llm = HuggingFaceHub(repo_id = \"google/flan-t5-large\", model_kwargs = {\"temperature\": 0, \"max_length\": 64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moscow\n"
     ]
    }
   ],
   "source": [
    "# Generate a response for a new question using Hugging Face and print it\n",
    "output = huggingface_llm.predict(\"Can you tell me the capital of Russia\")\n",
    "print(output)\n",
    "# Expected Output: \"Moscow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love the way i look at the world i love the way i feel i love the way i think i feel i love the way i feel i love the way i think i feel i love the way i feel i love the way \n"
     ]
    }
   ],
   "source": [
    "# Generate a response asking for a poem using Hugging Face and print the result\n",
    "output = huggingface_llm.predict(\"Can you write a poem about AI\")\n",
    "print(output)\n",
    "# Expected Output: (A short, generated poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenAI LLM to generate a poem response and print it\n",
    "openai_llm_output = openai_llm.predict(\"Can you write a poem about AI\")\n",
    "print(openai_llm_output)\n",
    "# Expected Output: (Another poem, possibly longer and more structured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PromptTemplate and LLMChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PromptTemplate:**\n",
    "- A PromptTemplate in LangChain is a tool for creating dynamic and structured prompts for language models. \n",
    "- It allows you to define placeholders for variables that can be replaced with user-provided input, ensuring consistency in how prompts are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me the capital of this India\n"
     ]
    }
   ],
   "source": [
    "# Importing PromptTemplate from LangChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template that expects a variable 'country' to be provided\n",
    "prompt_template = PromptTemplate(input_variables=['country'], template=\"Tell me the capital of this {country}\")\n",
    "\n",
    "# Use the prompt template to format the input with \"India\" as the country\n",
    "formatted_prompt = prompt_template.format(country=\"India\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LLMChain:**\n",
    "- LLMChain is a component that combines a language model (LLM) and a PromptTemplate to create a streamlined process for generating responses. \n",
    "- It takes in a user input, formats it with the prompt template, passes it to the LLM, and returns the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kathmandu\n"
     ]
    }
   ],
   "source": [
    "# Import LLMChain from LangChain, which links the LLM and the prompt template\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create an LLMChain object using the OpenAI LLM and the prompt template\n",
    "chain = LLMChain(llm=huggingface_llm, prompt=prompt_template)\n",
    "\n",
    "# Run the chain with \"India\" as input and print the response\n",
    "print(chain.run(\"Nepal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Combining Multiple Chain using Simple Sequence Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a template that accepts a country as input and generates a question asking for the capital.\n",
    "capital_template = PromptTemplate(input_variables=[\"country\"], template=\"Tell me the capital of {country} ?\")\n",
    "\n",
    "# Initialize a language model chain (LLMChain) with the Hugging Face model (or you can use OpenAI LLM) \n",
    "# and the capital_template. This chain will take a country name and generate the capital query.\n",
    "capital_chain = LLMChain(llm=huggingface_llm, prompt=capital_template)\n",
    "\n",
    "# Create another template that takes a capital as input and generates a question asking for tourist places.\n",
    "famous_template = PromptTemplate(input_variables=[\"capital\"], template=\"Tell me the places where I can go for visit, {capital}\")\n",
    "\n",
    "# Initialize a second LLMChain with the Hugging Face model (or OpenAI LLM) and the famous_template. \n",
    "# This chain will take a capital name and generate the places to visit query.\n",
    "famous_chain = LLMChain(llm=huggingface_llm, prompt=famous_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Temple of Vishnu\n"
     ]
    }
   ],
   "source": [
    "# Import SimpleSequentialChain from LangChain to chain the two models sequentially.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Combine the two chains (capital_chain and famous_chain) into a sequential chain. \n",
    "# The output of capital_chain (capital of a country) becomes the input to famous_chain (places to visit in the capital).\n",
    "chain = SimpleSequentialChain(chains=[capital_chain, famous_chain])\n",
    "\n",
    "# Run the sequential chain by providing \"India\" as the initial input. \n",
    "# The first chain will ask for the capital of India, and the second chain will ask for places to visit in that capital.\n",
    "print(chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sequentials Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a template that takes 'country' as an input variable and generates a question asking for the capital of the country.\n",
    "capital_template = PromptTemplate(input_variables=[\"country\"], template=\"Tell me the capital of {country} ?\")\n",
    "\n",
    "# Create the first language model chain (capital_chain) that uses the Hugging Face model and the capital_template.\n",
    "# It will output the capital city, and the result will be stored in the 'capital' key (using output_key).\n",
    "capital_chain = LLMChain(llm=huggingface_llm, prompt=capital_template, output_key=\"capital\")  # You can use openai_llm as well for more accurate results.\n",
    "\n",
    "# Create another template that takes 'capital' as an input variable and generates a query about famous places to visit in the capital city.\n",
    "famous_template = PromptTemplate(input_variables=[\"capital\"], template=\"Tell me the places where I can go for visit, {capital}\")\n",
    "\n",
    "# Create the second language model chain (famous_chain) that uses the Hugging Face model and the famous_template.\n",
    "# It will output famous places to visit in the capital, and the result will be stored in the 'places' key (using output_key).\n",
    "famous_chain = LLMChain(llm=huggingface_llm, prompt=famous_template, output_key=\"places\")  # You can use openai_llm here for more accurate results as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'India', 'capital': 'chennai', 'places': 'The Temple of Vishnu'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import SequentialChain from LangChain, which allows chaining multiple models sequentially, passing output from one chain to the next.\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# Combine the two chains (capital_chain and famous_chain) into a single sequential chain.\n",
    "# The input to the chain will be 'country', and the outputs will be both 'capital' and 'places'.\n",
    "# The capital_chain output will be passed as input to the famous_chain.\n",
    "chain = SequentialChain(chains=[capital_chain, famous_chain], input_variables=[\"country\"], output_variables=[\"capital\", \"places\"])\n",
    "\n",
    "# Execute the sequential chain by providing \"India\" as the input for 'country'.\n",
    "# The chain first asks for the capital of India, then asks for famous places to visit in the capital.\n",
    "chain({\"country\": \"India\"})  # Expected output with openai_llm would be accurate.\n",
    "\n",
    "# Example output (may not be accurate with Hugging Face LLM due to model limitations, but more accurate with OpenAI LLM):\n",
    "# {'country': 'India', 'capital': 'Chennai', 'places': 'The Temple of Vishnu'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==========> Comment: Hugging Face LLMs may not give accurate results as shown in the example ('Chennai' as capital instead of 'New Delhi').**\n",
    "\n",
    "**==========> If you use OpenAI LLM (which is more advanced), the outputs would be more accurate, such as providing 'New Delhi' as the capital.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
